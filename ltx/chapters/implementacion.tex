\chapter{Imlementación JPEG}\label{ch:implementacion}

Se implementó un codificador JPEG, lanzado al dominio público en github bajo el
nombre de TinyJPEG \cite{tiny_jpeg}. La ubicación permanente de esta biblioteca
está en \begin{alltt}https://github.com/serge-rgb/TinyJPEG \end{alltt}

Encima de TinyJPEG, se desarrolló un proyecto que extiende TinyJPEG para
implementar el algoritmo evolutivo \cite{gp_encoder}, localizado en
\begin{alltt}https://github.com/serge-rgb/gp_encoder\end{alltt}

En este capítulo se describen los detalles de ambos proyectos.

\section {Términos}
Se van a usar los términos \emph{debugging} y \emph{profiling} por su ubiquidad
en la literatura. \emph{debugging} se traduce como depurar y se define como el
proceso de encontrar y arreglar defectos de código. \emph{profiling} es el
proceso de encontrar los puntos en los que un programa puede ser modificado
para mejorar el desempeño.

\emph{General Purpose GPU}, o \emph{GPGPU} es el
término usado para la práctica de usar programar GPUs directamente, a
diferencia de el uso original, en el cual el GPU era un acelerador cuya
interfaz era una biblioteca de gráficas como OpenGL o DirectX.

% ============================================================
\section{Tecnologías}
% ============================================================

La elección de lenguaje para TinyJPEG fue C, en particular C99 \cite{c99}.
TinyJPEG fue lanzado con el objetivo de ser una biblioteca reutilizable por
otras personas, y ha encontrado cierto grado de éxito. Un planetario en París
usa TinyJPEG para capturar vídeo de sus simulaciones.

C es un lenguaje ideal para escribir cosas como codificadores. El lenguaje
permite mantener el nivel bajo de abstracción que se necesita y las
herramientas de \emph{debugging} son mejores para C y C++ que para casi
cualquier otro lenguaje.

Se escogió OpenCL para la implementación de GPGPU. OpenCL es un estándar abierto
para GPGPU y está soportado por Intel, Nvidia y AMD. La máquina en la que se
implementó este trabajo tiene una tarjeta Nvidia. Nvidia tiene su propio
lenguaje para GPGPU, llamado CUDA, que tiene mejor soporte para debugging y
profiling que OpenCL para sus tarjetas de video. Sin embargo, las herramientas
siguen siendo primitivas comparadas con lo que se tiene en el CPU. En cualquier
caso, uno termina haciendo hipótesis, experimentos y medidas para optimizar la
solución, aún teniendo herramientas sofisticadas. Se escogió OpenCL porque los
méritos relativos de facilidad de desarrollo no le ganan al soporte
multi-plataforma y al valor de apoyar estándares abiertos.

% ============================================================
\section{Arquitectura}
% ============================================================

TinyJPEG es minimalista. Consiste de un archivo de alrededor 1000 líneas. Está
escrito en el estilo popularizado por Sean Barrett de escribir un solo archivo
\verb+.h+ con la siguiente estructura:

\label{alg:stb}
\begin{code}[language=C][h]
    // Principio del archivo
    #pragma once

    // Definición de la interfaz.
    tje_encode_to_file(...);

    #ifdef TJE_IMPLEMENTATION

    // La implementación completa va aquí.
\end{code}

De esta manera, uno puede incluir \verb+#include <tiny_jpeg.h>+ como cualquier
\emph{header} de C, pero en uno de los archivos del proyecto, se hace esto:

\label{alg:stb_impl}
\begin{code}[language=C][h]
    #define TJE_IMPLEMENTATION
    #include <tiny_jpeg.h>
\end{code}

para definir la implementación completa. El propósito de esta técnica es
facilitar la distribución de bibliotecas en un lenguaje que no cuenta con un
sistema de distribución digital de módulos,
o siquiera un sistema de módulos.

Se exponen dos funciones como interfaz pública:

\begin{code}[language=C][h]
tje_encode_to_file(...)
tje_encode_to_file_at_quality(...)
\end{code}

La primera comprime con la tabla unitaria y la segunda ofrece tres posibles
niveles de calidad, todos correspondientes únicamente al uso de tres tablas de
cuantificación. Ambas funciones son \emph{wrappers} sobre la función principal
interna, que funciona de la siguiente manera:

Se prepara una estructura estática para hacer compresión de Huffman y se
pre-procesa la tabla según el algoritmo \ref{alg:fast-dct}, descrito en la
siguiente sección. A esto le llamamos el \emph{paso inicial}.
Cuando terminamos el \emph{paso inicial}, determinamos el número de bloques que
van a ser procesados. JPEG debe funcionar con imágenes cuyos tamaños verticales
y horizontales no son múltiplos de 8. Para esto la especificación sólo nos pide
como codificador redondear al siguiente múltiplo de 8. Al decodificador le pide
ignorar los datos extra. Para evitar artefactos, como convención se repite en
el bloque el color del último píxel de la imagen para cada columna extra y para
cada renglón extra. Si $w$ es el ancho de la imagen y $h$ es el alto, entonces
el número de bloques es
$n = (w + (8 - w \mod 8)) * (h + (8 - h \mod 8))$

Cada bloque se separa en tres bloques \verb+Y+, \verb+U+ \verb+V+ que a los que
se les aplica la función \verb+encode_MCU()+.

La función \verb+encode_MCU()+ es llamada así por el acrónimo \emph{MCU},
\emph{Minimum Coded Unit}. JPEG puede especificar un factor para describir los
bloques \verb+U+ y \verb+V+ con menos resolución, por nuestra relativa falta de
sensibilidad a la crominancia contra la luminancia, pero TinyJPEG no utiliza
esto. Otra posibilidad es usar diferentes tablas de codificación para
lumninancia y crominancia. TinyJPEG sí utiliza esto, escogiendo tablas con
mayor compresión para los bloques de crominancia.

El flujo de la lógica es parecido a como se describe en Español.
Un \verb+for loop+ para extraer los bloques, y cada bloque se pasa como
parámetro a la función \verb+encode_MCU+.

\subsection{DummyJPEG} \label{sub:dummy}

A primera vista, el algoritmo JPEG se ve ``embarazosamente paralelo". Una
motivación para este trabajo fue la observación de que el algoritmo trabaja
dividiendo la imágenes en cuadros de $8\times8$, y el hecho de que los GPUs
actuales trabajan con instrucciones vectoriales de 64 elementos.

Desafortunadamente, el algoritmo JPEG no es paralelo. Aplicar codificación
delta al coeficiente DC introduce una dependencia de datos entre cada bloque de
\verb+Y+, \verb+U+ y \verb+V+ respectivamente. Para cada componente, cualquier
bloque después del primero depende del anterior para poder computar la
diferencia entre su coeficiente DC  y el de su antecesor.

Sin embargo, aunque JPEG es inherentemente secuencial para cada componente,
está muy cerca de ser paralelo. Si el \emph{Joint Photographic Experts Group}
no hubiera decidido tratar de manera diferente a los coeficientes DC y AC, el
algoritmo sería completamente paralelo.

Queremos darle la vuelta al problema, y la manera en que lo hacemos es creando
un "Dummy JPEG": Un algoritmo que es \emph{casi} JPEG, pero que no es correcto.

La implementación de DummyJPEG empieza como un clon directo de TinyJPEG. Lo
primero que hacemos es cambiar las función que escribe a disco por una función
que va contando el número de bits. De esta manera, al aplicar el algoritmo no
tenemos una imagen, pero tenemos un reporte del tamaño de la imagen que
hubiéramos generado.

También cambiamos el algoritmo para aplicar la Transformada Inversa de Coseno
justo después de aplicar la DCT al bloque, para poder compararlos y calcular el
error.

Entonces, si nuestro algoritmo original para cada bloque en TinyJPEG es:

\begin{code}
    x = aplicar_dct(bloque);
    codificar(x)
\end{code}

Nuestro algoritmo para cada bloque en DummyJPEG se convierte en esto:

\begin{code}
    x = aplicar_dct(bloque)
    reportar_tamaño(x)
    y = aplicar_idct(x)
    reportar_error(bloque, y)
\end{code}

En donde \verb+reportar_error+ es una suma de la diferencia absoluta entre cada
pixel del bloque original y del bloque reconstruido.

La manera en que volvemos paralelo a DummyJPEG es simplemente no codificar al
coeficiente DC. Esto introduce un error en el tamaño reportado pero no afecta
al error reportado. El error que introducimos viene de que contamos los bits de
la representación de los 63 coeficientes AC, pero no contamos los bits del
coeficiente DC. En las imágenes de prueba que se usaron para este trabajo, el
tamaño reportado es menor que el tamaño real entre un 10\% y 20\%. El error es
proporcionalmente más alto para imágenes con más energía en el primer
coeficiente.

La pregunta que nos hacemos es: ¿El error en el reporte de tamaño es
significativo?

Las presión que ponemos en la evolución es hacia imágenes que son
indistinguibles de la original. Como el valor de cuantificación del primer
coeficiente afecta de manera importante a la calidad de imagen comprimida, las
tablas de la población rápidamente convergen a tener el primer valor de
cuantificación igual a 1. Esto quiere decir que para casi cualquier conjunto de
tablas que vayamos a comparar, estamos introduciendo el mismo error en el
reporte de los tamaños de las imágenes resultantes. Por lo tanto, podemos
despreocuparnos por completo de no tomar en cuenta el primer coeficiente.

% Describir el deseo de hacer una implementación paralela en el CPU
Ya que tenemos un algoritmo que es \emph{casi JPEG}, pero paralelo, seguimos
con la tarea de cambiar la arquitectura del programa para paralelizarlo en el
CPU. La razón por la que implementamos el paralelismo en el CPU antes de
hacerlo en el GPU es principalmente la velocidad de desarrollo. La manera en
que implementamos la versión paralela en CPU se hace con \emph{plan con maña}.
Moldeamos el código para que el flujo de ejecución en el CPU sea muy
similar a la manera en que trabaja el GPU, intentando llegar al punto de que la
implementación en el GPU se reduzca a un \emph{copy paste}, y que el esfuerzo
que se aplique a la implementación del GPU sea solamente un trabajo de
optimización de bajo nivel.

\section{Introducción a GPGPU}

Las arquitecturas de GPU, en el momento que se escribe esto, trabajan con
instrucciones SIMD (\emph{Single Instruction, Multiple Data}), también llamadas
\emph{instrucciones vectoriales}.

Supercomputadoras vectoriales como la \emph{Cray} se introdujeron en los 70s y
gradualmente perdieron popularidad mientras computadoras con microprocesadores
x86 bajaron de precio.

En 1998 AMD introdujo instrucciones vectoriales con su tecnología \emph{3D
Now}, seguido por Intel en 1999 cuando introdujo SSE (Streaming SIMD
extensions). Ambas tecnologías son extensiones al conjunto de instrucciones x86.

Por esa misma época empezaron a salir aceleradores gráficos: tarjetas que se
dedicaban a rasterizar triángulos rápidamente para aplicaciones multimedia,
principalmente videojuegos.

Aunque al principio los GPUs estaban diseñados alrededor de APIs como OpenGL y
DirectX, con funcionalidad fija, poco a poco fueron adquiriendo la habilidad de
ser programables. Primero con \emph{shaders}, que permitieron meter código en
un par de partes clave del proceso de rasterización, y más tarde con OpenCL y
CUDA, que permiten escribir programas para el GPU en un dialecto de C.

El modelo de ejecución de OpenCL se mapea directamente a las arquitecturas GPU
actuales y funciona de la siguiente manera.

Un programa OpenCL opera con \emph{work items}. Un work item puede pensarse
como el apuntador de instrucciones en x86, pero que ejecuta solamente
instrucciones vectoriales. En la arquitecturas modernas de Nvidia, la longitud
de los vectores es de 32 elementos.

Los \emph{work items} corren \emph{kernels}, que son esencialmente funciones
escritas en el dialecto de C de OpenCL escritas para ser ejecutadas en el GPU.

Un \emph{work item} es parte de un \emph{work group}. El tamaño del \emph{work
group} es decisión del programador, puede ser de 1, 2 o 3 dimensiones y su
tamaño puede variar, aunque es buena práctica escoger un múltiplo del tamaño
vectorial (32 en Kepler y Maxwell). Hay que encontrar el tamaño apropiado de
\emph{work group} para minimizar contención de memoria y maximizar el
rendimiento (traducción de \emph{throughput}).

La tarjeta de video tiene un calendarizador implementado en hardware que
trabaja con granularidad de work items individuales. El \emph{work group} es
una abstracción que se provee para proveer al \emph{kernel} con índices
relativos a la dimensión y tamaño del espacio de \emph{work groups}. Esto es
útil para que el programador diseñe el algoritmo para utilizar la jerarquía de
memoria eficientemente y evitar ejecución condicional. Es útil usar referencias
oficiales \cite{maxwell-tuning} o buscar los recursos gratis de GPU Tech Conf,
una conferencia anual sobre GPUs en la que siempre hay pláticas que explican
tips y trucos para sacar el mayor desempeño posible de las arquitecturas
actuales. \cite{gtc}

Cada work item tiene un número limitado de registros y un espacio de memoria
local (a veces llamada stack, por su equivalente operacional en CPUs). Cada
work group tiene \emph{memoria compartida}, con acceso de lectura y escritura
para cualquier miembro del grupo. Todos los work groups tienen acceso de
lectura y escritura a la memoria global. El CPU también tiene acceso de lectura
y escritura a este espacio de memoria, y es por aqui donde se especifican datos
de entrada para el kernel y de donde se leen los resultados.

Hay una correspondencia entre el modelo de memoria de OpenCL y la jerarquía de
memoria de las arquitecturas GPU. Existe una memoria compartida con garantía de
coherencia, y un número de \emph{cores} capaces de ejecutar work items existen en
grupos de tamaño fijo con memoria local.

La memoria global puede ser copiada automáticamente a memoria local, de la
misma manera que se utilizan cachés en CPUs para lidiar con la relativa
lentitud de la memoria RAM.

Al programar GPUs es muy fácil entrar a problemas de contención de memoria. La
arquitectura está diseñada para lidiar con contención. Cuando un work item está
esperando a que se efectúe una operación de memoria, el calendarizador hace que
el \emph{core} ejecute otro work item que esté listo para ser ejecutado.

Cuando se sabe que un work item va a trabajar con un trozo de memoria de cierto tamaño, es buena práctica hacer una copia a memoria local. El siguiente pseudocódigo es una optimización común en OpenCL.

\label{alg:gpgpu-memcpy}
\begin{code}[language=C][h]
    // 64 es un número arbitrario.
    // Hay que tomar en cuenta el tamaño de la memoria local.
    int mi_copia[64];
    for (int i = 0; i < 64; ++i) {
        mi_copia[i] = memoria_global[i];
    }
\end{code}

El kernel para nuestra función de selección va a hacer el trabajo de la función
\verb+encode_MCU+, que se encarga de tomar bloques de $8\times8$, codificarlos
y reportar el tamaño, y luego decodificarlos y reportar el error.

Lo diseñamos para que cada warp se encargue de un bloque. Se llama \emph{una sóla vez} para todos los bloques, en el caso de un sólo
\emph{thread}. para múltiples threads, se dividen los arreglos de bloques entre
el número de trabajadores, y se la función se llama una vez por cada cpu.
explicar que estamos evoucionando una tabla:

sólo procesa los bloques de luminancia. el algoritmo evolutivo genera una tabla
por imagen. usar la lumninancia exclusivamente requiere un tercio del trabajo y
resulta en un cambio negligible en el reporte del error.

el cambio arquitectural de dummyjpeg no sólo facilita la paralelización en cpu,
sino que hace posible la implementación del algoritmo paralelo en el gpu
\ref{sec:gpgpu}. dummyjpeg procesa $n$ bloques a la vez y escribe a un arreglo
de $n$ resultados. un resultado es una tupla $(b, e)$ donde $b$ es el número de
bits que el bloque consume después de ser codificado (con un ligero error
inducido al ignorar el coeficiente dc) y $e$ es la suma de las diferencias
absolutas pixel-por-pixel entre el bloque original y el bloque descomprimido.

el algoritmo paralelo trabaja en un patrón \emph{map-reduce}. la función \emph{map} es el proceso paralelo que acabamos de describir, y el \emph{reduce} consiste de:

\begin{code}
    B_total = 0;
    E_total = 0;
    Para todo resultado (B, E):
       B_total += B;
       E_total += E;
    Terminar el algoritmo, regresando (B_total, E_total).
\end{code}

La tupla que regresa DummyJPEG son los parametros a la función de selección
\ref{eq:fitness}.

% ============================================================
\section{Algoritmo DCT}
% ============================================================

A partir de la ecuacion \ref{eq:dct} se puede derivar directamente un algoritmo
simple para la \emph{DCT}:

\label{alg:dct}
\begin{code}[language=C][h]
    float DCT[64];
    for (int v = 0; v < 8; ++v) {
        for (int u = 0; u < 8; ++u) {
            DCT[v*8 + u] = F(u, v);
            // F es la traducción directa de definición DCT
    }
\end{code}

\verb+tiny_jpeg+ contiene dos implementaciones de \emph{DCT}, la que se deriva
directamente de la ecuación \ref{eq:dct} y que se describe en \ref{alg:dct}
muestra arriba, y una más rápida, desarrollada por \cite{ahmed_dct}.

El algoritmo \ref{alg:dct} no es práctico para un codificador JPEG y mucho
menos para este proyecto, que pretende ejecutar el algoritmo JPEG
potencialmente miles de veces para una sola imagen. Existen métodos calcular la
DCT (y su inversa, la IDCT) rápidamente. La discusión presentada aquí sobre el
desarrollo de algoritmos rápidos DCT es análoga para la inversa, ya que las
ecuaciones son muy similares (\ref{eq:dct}, \ref{eq:idct}).

Los algoritmos rápidos para calcular la \emph{DCT} están basados en la
observación de que la ecuación \ref{eq:dct} es lineal, y por lo tanto el
cálculo \emph{DCT} se puede expresar como $F(X) = A^{T}XA$ Donde $X$ es un bloque de $8\times8$ y A es la matriz:

\begin{equation}
    \label{eq:dct-matrix}
    \begin{bmatrix}
        \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \\
        cos\frac{\pi}{16} & cos\frac{3\pi}{16}& cos\frac{5\pi}{16}& cos\frac{7\pi}{16}& cos\frac{9\pi}{16}& cos\frac{11\pi}{16}& cos\frac{13\pi}{16}& cos\frac{15\pi}{16} \\
        cos\frac{2\pi}{16} & cos\frac{6\pi}{16}& cos\frac{10\pi}{16}& cos\frac{14\pi}{16}& cos\frac{18\pi}{16}& cos\frac{22\pi}{16}& cos\frac{26\pi}{16}& cos\frac{30\pi}{16} \\
        cos\frac{3\pi}{16} & cos\frac{9\pi}{16}& cos\frac{15\pi}{16}& cos\frac{21\pi}{16}& cos\frac{27\pi}{16}& cos\frac{33\pi}{16}& cos\frac{39\pi}{16}& cos\frac{45\pi}{16} \\
        cos\frac{4\pi}{16} & cos\frac{12\pi}{16}& cos\frac{20\pi}{16}& cos\frac{28\pi}{16}& cos\frac{36\pi}{16}& cos\frac{44\pi}{16}& cos\frac{52\pi}{16}& cos\frac{60\pi}{16} \\
        cos\frac{5\pi}{16} & cos\frac{15\pi}{16}& cos\frac{25\pi}{16}& cos\frac{35\pi}{16}& cos\frac{45\pi}{16}& cos\frac{55\pi}{16}& cos\frac{65\pi}{16}& cos\frac{75\pi}{16} \\
        cos\frac{6\pi}{16} & cos\frac{18\pi}{16}& cos\frac{30\pi}{16}& cos\frac{42\pi}{16}& cos\frac{54\pi}{16}& cos\frac{66\pi}{16}& cos\frac{78\pi}{16}& cos\frac{90\pi}{16} \\
        cos\frac{7\pi}{16} & cos\frac{21\pi}{16}& cos\frac{35\pi}{16}& cos\frac{49\pi}{16}& cos\frac{63\pi}{16}& cos\frac{77\pi}{16}& cos\frac{91\pi}{16}& cos\frac{105\pi}{16}
    \end{bmatrix}
\end{equation}

Esta matriz tiene alta redundancia gracias a la simetría del coseno y al factor $16$. Por ejemplo, el cuarto elemento del segundo renglón es $cos\frac{7\pi}{16} \approx 0.19509$ igual al siguiente elemento del renglón salvo al signo: $cos\frac{9\pi}{16} \approx -0.19509$, que es igual al último elemento de la matriz: $cos\frac{105\pi}{16} \approx -0.19509$. Siguiendo este proceso simplificamos la matríz:

\begin{equation}
    \label{eq:dct-matrix-simple}
    \sqrt{2}/2
    \begin{bmatrix}
        1 & 1 & 1 & 1 & 1 & 1 & 1 & 1  \\
        a & c & d & f & -f & -d & -c & -a \\
        b & e & -e & -b & -b & -e & e & b \\
        c & -f & -a & -d & d & a & f & -c \\
        1 & -1 & -1 & 1 & 1 & -1 & -1 & 1\\
        d & -a & f & c & -c & -f & a & -d \\
        e & -b & b & -e & -e & b & -b & e \\
        f & -d & c & -a & a & -c & d & -f
    \end{bmatrix}
\end{equation}

donde

\begin{eqnarray*}
    a = \frac{2}{\sqrt{2}}cos\frac{\pi}{16}\\
    b = \frac{2}{\sqrt{2}}cos\frac{\pi}{8}\\
    c = \frac{2}{\sqrt{2}}cos\frac{3\pi}{16}\\
    d = \frac{2}{\sqrt{2}}cos\frac{5\pi}{16}\\
    e = \frac{2}{\sqrt{2}}cos\frac{3\pi}{8}\\
    f = \frac{2}{\sqrt{2}}cos\frac{7\pi}{16}
\end{eqnarray*}

Entonces, una columna Y del bloque procesado por DCT se puede descomponer así:

\begin{equation}
    \label{eq:dct-row}
    \begin{bmatrix}
        Y(0) \\
        Y(2) \\
        Y(4) \\
        Y(6)
    \end{bmatrix}
    = \frac{\sqrt{2}}{2} \begin{bmatrix}
        1 & 1 & 1 & 1  \\
        b & e & -e & -b \\
        1 & -1 & -1 & 1  \\
        e & -b & b & e
        \end {bmatrix} \begin {bmatrix}
        X(0) + X(7) \\
        X(1) + X(6) \\
        X(2) + X(5) \\
        X(3) + X(4)
        \end {bmatrix}
\end{equation}

\begin{equation*}
    \begin{bmatrix}
        Y(0) \\
        Y(2) \\
        Y(4) \\
        Y(6)
    \end{bmatrix}
    = \frac{\sqrt{2}}{2} \begin{bmatrix}
        a & -c & d & -f  \\
        c & f & -a & d  \\
        d & a & f & -c  \\
        f & d & c & a
        \end {bmatrix} \begin {bmatrix}
        X(0) - X(7) \\
        X(6) - X(1) \\
        X(2) - X(5) \\
        X(4) - X(3)
        \end {bmatrix}
\end{equation*}

donde $X$ es un renglón del bloque.

Por lo tanto, si aplicamos la ecuación \ref{eq:dct-row}, avanzando columna por columna por el bloque original, obtenemos el bloque DCT.

El algoritmo resultante es lo suficientemente rápido para no aparecer como un cuello de botella significativo al momento de hacer optimización.

% ============================================================
\section{Manejo de memoria}
% ============================================================

TinyJPEG no efectúa alojamiento dinámico de memoria. Aloja suficiente memoria en el \emph{stack} para guardar las tablas de Huffman y por razones de eficiencia también mantiene un \emph{buffer} de 8KB, que se usa para minimizar llamadas a sistema.

Para el algoritmo paralelo, no es posible mantener esta propiedad. TinyJPEG procesa la imagen un bloque a la vez, mientras que DummyJPEG recibe $N$ bloques y escribe $N$ resultados en paralelo. Esto forza al sistema a alojar memoria para los bloques y para los arreglos donde se guardan los resultados.

Si OpenCL se utiliza, DummyJPEG aloja memoria en el GPU para los arreglos de entrada y de salida, así como para las tablas de Huffman. La latencia es el punto débil de las arquitecturas GPU. Por razones de eficiencia, los arreglos que se usan una vez por imagen son definidos una sola vez. Esto incluye los arreglos de los bloques, las tablas de Huffman. Los únicos datos que deben ser actualizados en cada llamada al \emph{kernel} son la tabla DCT y los arreglos de resultados.


% ============================================================
\section{Optimización}
% ============================================================

% ============================================================
\section{GPGPU} \label{sec:GPGPU}
% ============================================================
